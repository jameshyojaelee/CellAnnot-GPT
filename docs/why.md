1. GPT-cell-annotator – “What cell type is this? Let AI decide.”
	•	Pitch: An AI assistant that automatically annotates cell types in single-cell data by examining marker genes and consulting a large knowledge base (cell atlases, literature) ￼. It provides suggested labels with explanations, accelerating a task that currently takes human experts days per dataset.
	•	Target User & Context: Single-cell RNA-seq analysts (grad students, computational biologists) who have clustered their data and want quick, evidence-backed cell type labels. Buying context: could be offered as a web app or plugin where users upload gene signatures; academics could use a free/basic version, whereas industry teams might pay for a self-hosted or API version for proprietary data.
	•	Admissions Impact (10/10): Extremely high. This demo screams innovation: applying cutting-edge LLM tech to a current biology problem. Faculty on admissions committees will recognize the challenge of cell annotation ￼ and be impressed by a solution leveraging AI. The concept aligns with emerging research (some very recent papers are exploring it ￼), showing the candidate is at the forefront. It also demonstrates the candidate’s proficiency in both biology and AI, a hot combination.
	•	4-Week Feasibility (7/10): Moderate-high. A basic version can be built by combining existing components: e.g., use GPT-4 or llama2 with a prompt that includes top marker genes of a cluster and ask for cell type suggestion ￼. Fine-tuning a smaller model on known annotated datasets (like CellTypist’s database or CellMarker) is possible within weeks. The knowledge (cell ontology, marker gene lists) can be encoded via prompt or a vector database. The biggest challenge is evaluation: ensuring the AI’s suggestions make sense biologically. But a limited prototype (for, say, immune cells and a few tissues) is doable. We might leverage an existing open-source model (there is one called scGPT for single-cell data) for embedding gene expression, plus a GPT layer for interpretation. Using an LLM coding agent in development can speed up integration of data sources.
	•	Monetization (7/10): Potential is there. Freemium SaaS model: free for small academic use (e.g., limit on cells or clusters), paid tier for full datasets or API access. Biotech companies working in single-cell (who value scientists’ time) could pay for a tool that cuts annotation time. Another revenue path: integrate it into platforms like 10x Genomics or Cellxgene – licensing the model. Since cell annotation is a common bottleneck, even consulting services currently fill this gap (people pay experts to annotate) – an AI tool could capture some of that value. One risk: if an academic group releases a free model, it could compete. But we can differentiate with a polished interface and continuous updates.
	•	Competitive Alternatives: Traditional: CellTypist (ML model) and SingleR which use reference-based approach ￼. Emerging: the multi-LLM consensus paper ￼, and others like scAnnotate (if any appear). However, no user-friendly AI assistant exists yet – alternatives require coding or have limited scope. For instance, CellTypist has a command-line tool and fixed reference sets; our LLM could flexibly handle custom clusters and incorporate literature on the fly. We should cite that even current best tools (CellTypist, scANVI) sometimes miss cell types or require reference data the user may not have ￼, whereas an LLM can draw from broader context (if properly equipped).
	•	Unique Wedge: Knowledge integration. This product’s edge is using a large language model that “knows” about cell types in a broad sense – including rare ones described only in literature. It can say “This cluster looks like alveolar macrophages because markers A, B, C match known profiles ￼.” That explanatory ability is novel and valuable for trust. Evidence it matters: users explicitly ask for more “comprehensive” ways to annotate ￼, and are currently stringing together multiple tools to be sure ￼. An AI that consolidates that would hit a sweet spot.
	•	Risks & Mitigations: The major risk is accuracy – an LLM might hallucinate or confidently assign wrong labels, which could mislead users. Mitigation: restrict the model’s domain via prompt (e.g., force it to pick from known ontology terms if confidence is low, or output a “unknown” if unsure, similar to scArches’ ability to say “no match” ￼). We’d also include a validation step: cross-check AI’s answer with known marker databases (if AI says “cell type X”, ensure the top markers overlap with literature markers for X). Another risk is adoption – biologists might be skeptical of AI. We can mitigate by making outputs transparent (provide reasoning and a confidence score, and references to sources where possible). Finally, dependency on external APIs (if using GPT-4) could be an issue; to mitigate, we’d plan a local model version (maybe fine-tuned smaller LLM) for sustainability.


Problem & User Persona: Researchers with single-cell transcriptomic data struggle to annotate cell types for each cluster, especially when they lack extensive domain knowledge or when dealing with novel cell populations ￼. The process is manual and time-consuming, requiring comparing marker genes to literature. Our users are computational biologists or wet-lab scientists who have obtained a clustered single-cell dataset (e.g., via Seurat or Scanpy) and now need to label those clusters meaningfully. They often face deadlines (writing a paper, reporting results) and would benefit from a quick, reliable suggestion system. These users value accuracy and explanations (to trust the suggestion), and many are not AI experts – so the tool must be simple to use and not a black box.

Jobs-to-be-Done:
	•	Primary Job: Given a set of marker genes (or an expression profile) for a cluster of cells, identify the likely cell type (e.g., “CD8+ memory T cell”) with an explanation.
	•	Secondary Job: Highlight if a cluster does not match any known cell type (novel population detection) ￼.
	•	Secondary Job: Speed up the annotation of large datasets by handling bulk queries (label all clusters in a dataset automatically).
	•	Emotional Job: Give less-experienced users confidence that their annotations are grounded in known biology, reducing anxiety of missing something ￼.

Proposed Solution: A web or desktop application where users can upload cluster marker gene lists (or a processed single-cell object) and receive suggested cell type labels. The core engine uses a large language model (LLM) fine-tuned or prompted on cell-type knowledge: it has been fed known marker genes from references (like CellMarker, PanglaoDB, Human Cell Atlas data) and relevant literature. When a query comes (e.g., cluster has markers X, Y, Z highly expressed), it outputs: “Likely cell type: Alveolar Macrophage. Explanation: markers X and Y are classic for alveolar macrophages ￼, which are lung-resident macrophages. Marker Z suggests an activated state.” along with a confidence score. The UI will allow the user to accept the suggestion or see alternatives. There will also be an option to provide context (if the tissue or species is known, to constrain the suggestions). Non-goal: the tool is not meant to be a final authority; it won’t automatically rewrite the user’s analysis or integrate into clustering – it operates post-clustering. It’s also not designed for single-cell multiome (combined modalities) in the first version – only transcriptomic profiles, possibly protein if provided.

Scope (Features):
	•	Input acceptance: list of marker genes for a cluster (with optional metadata like up/down indicators, average log-fold changes). Alternatively, the user can select a cluster ID from an uploaded labeled matrix; the tool will compute top markers itself.
	•	AI annotation engine: LLM (e.g., GPT-4 via API) with a crafted prompt that includes marker genes and optionally their expression specificity. The prompt will also contain a condensed knowledge base (e.g., a few-shot examples or a summary of known markers from our database).
	•	Output: For each cluster, a suggested label (cell type name from a known ontology or common usage) and an explanation sentence or two. Possibly also an ontology ID if available (for standardized use).
	•	Feedback mechanism: User can mark suggestion as correct or incorrect – if incorrect, they can input what it was (this can help improve the model over time, but that’s beyond 4-week MVP scope except storing the feedback).
	•	Batch mode: the user can get suggestions for multiple clusters in one go, e.g., if 10 clusters, it lists suggestions for each.
	•	Explanation & source: If possible, cite a source for the marker gene association (e.g., a reference or database entry) ￼. We could have a backend database of marker→cell type mappings and have the LLM or code fetch a reference link.
	•	UI/UX: Simple form or file upload interface. Perhaps a text area to paste markers or a button “Analyze” if the object is uploaded. Show results in a table: Cluster name -> Suggested Type -> Confidence (High/Med/Low) -> Explanation.
	•	Non-Goals/Exclusions: The tool will not do de novo clustering or differential expression itself (assumes that is done). It also will not output fully detailed cell ontology mappings in v1, aside from maybe a known label – integration with ontologies could be future scope. It won’t handle extremely rare cell types beyond its knowledge (if none found, it will say “Unknown or novel”). It’s not a substitute for careful validation (we won’t, for example, guarantee 100% correctness or pick up subtle differences like sub-subtypes without clear markers).

User Stories & Acceptance Criteria:
	1.	Story: As a lab scientist with a new scRNA-seq dataset of blood cells, I want to quickly label each cluster so that I can interpret my UMAP plot for a meeting tomorrow.
	•	Acceptance: After uploading my cluster markers, the system returns labels like “Naive B cell”, “CD4+ T cell (naive)”, “CD4+ T cell (memory)” etc., with >80% of them matching what a human expert later confirms. Each label comes with a rationale (e.g., “Cluster 3: CD4 memory T cell because of markers IL7R, CCR7 low, and high IFNG”). The interface lets me copy these labels and explanations.
	2.	Story: As a computational biologist, I want to check if any clusters in my brain single-cell data might be novel cell types not in references, so I can flag them for deeper analysis.
	•	Acceptance: The tool processes all clusters; for one cluster, it outputs “No well-matching known cell type (top guess: Astrocyte-like, but with atypical marker ABC).” This signals to me this cluster might be something novel. The others have confident matches (neurons, oligodendrocytes, etc.) with high confidence.
	3.	Story: As a new grad student unfamiliar with immunology, I have a cluster with markers I recognise (like CD3E, CD8A, GZMB) but I’m unsure what subtype it is. I want an explanation I can trust to report.
	•	Acceptance: For that cluster, the tool returns “Cytotoxic CD8+ T lymphocyte” with explanation “expresses CD8A and GZMB indicating cytotoxic T cells; CD3E confirms T lymphocyte” and maybe cites a known immunology reference. This matches what my PI expected and saves me from searching textbooks.
	4.	Story: As a user, I want to easily provide the input without much formatting fuss.
	•	Acceptance: I can simply copy-paste a comma-separated gene list or upload a CSV with two columns (gene and avg_logFC) for each cluster. The system processes it without errors and doesn’t require me to learn some new format.

Week-4 Demo Metrics: (assuming we do a live or video demo by end of month)
	•	Accuracy/Validation: For a test set of say 5 known datasets (with expert-labeled cell types), the tool’s primary suggestion matches the expert label for ≥70% of clusters ￼. (We can show examples in the demo, e.g., a PBMC dataset where it correctly labels all main cell types – T, B, NK, monocyte, etc.)
	•	Runtime: The system provides results within, say, 1 minute for a dataset of 10 clusters (some overhead calling GPT-4). For demo, we aim for ~10 seconds per cluster with caching of common patterns.
	•	User Engagement: In a live trial before demo, we aim to have at least 3 different users (maybe lab mates) try it and give feedback, and incorporate any usability fixes (ease of input, clarity of output). Metric: positive feedback from these trial users like “this saved me time” or evidence they identified a correct cell type they initially mis-annotated.
	•	Robustness: The demo will specifically show one case where the tool admits uncertainty (“Unknown cluster”) to illustrate it’s not just guessing – a metric is handling such cases gracefully 100% (i.e., no obviously wrong label with high confidence – better to output unknown).
	•	Sign-ups/interest (if web): If we have a landing page by week 4, metric could be number of people who tried the live demo or signed up for updates (goal: at least 10 interested users, perhaps from an online forum post). This would show traction potential.

Technical Plan: (Leveraging an LLM coding agent in development)
	•	Architecture: Backend will be Python (for easy ML integration and possibly using an open LLM if needed). The LLM itself could be OpenAI GPT-4 via API initially (for best language quality) ￼. If API costs or privacy are concerns, a local model like LLaMA2 fine-tuned on cell type data (~7B or 13B params) could be attempted, but likely GPT-4 or GPT-3.5 will be used for the 4-week prototype. A small database of known markers will be compiled (from sources like PanglaoDB, CellMarker, and manual curation of literature).
	•	Data sources: We will utilize: PanglaoDB (700 cell types with marker genes), Human Cell Atlas annotations, immune cell marker lists from papers, etc., to ground the model. Possibly generate synthetic Q&A pairs for fine-tuning: e.g., prompt: “Markers: CD19, MS4A1, CD79A. Which cell type?” answer: “B cell (CD19+, CD20+ are B-cell markers)…”. Fine-tuning might be heavy; a simpler approach is providing these as few-shot examples to GPT-4 in the prompt.
	•	LLM Prompt Strategy: A likely design: create a prompt template with sections: (a) Known context: a brief list of known marker→cell mappings relevant (dynamically fetched: e.g., if many immune markers, include immune context). (b) The task: “Determine cell type from marker genes.” (c) Examples: [some demonstration of format]. (d) The query markers. We will use the LLM’s ability to reason and find best matches.
	•	Libraries & Stack:
	•	Web framework: Possibly Streamlit or Flask for a quick UI. Streamlit can allow easy file upload and real-time display, good for a demo.
	•	LLM API: openai library for GPT calls, or Huggingface’s transformers if using local model.
	•	Data handling: Pandas for reading inputs, maybe anndata for single-cell objects if we integrate that.
	•	If fine-tuning needed: use Huggingface Trainer on a smaller model, with our Q&A dataset. But likely unnecessary due to time and GPT-4’s competence with good prompting.
	•	Integration with coding agent: The plan is to use the LLM coding agent to expedite writing code for reading various input formats (e.g., it can generate code to parse a Seurat .rds file by calling R in background or converting to loom). Also, to generate the front-end UI boilerplate. For example, prompt GPT-4: “Write a Streamlit app that accepts a CSV of marker genes and outputs a table of suggestions by calling an API function annotate_cluster(markers).” We’ll then fill annotate_cluster with the actual call to GPT for each cluster. Use the agent for testing scenarios too (unit tests).
	•	Licensing & Data Constraints: We must ensure any marker gene database we use is open (PanglaoDB is CC0, I believe – will verify). If we fine-tune an open model with that data, license should be okay (the data is compiled from literature). The output knowledge is general so no major IP concerns. Using OpenAI API, we’ll need to not send sensitive data unless the user consents (though gene names aren’t sensitive by themselves, but it could imply tissue or condition – we’ll mention in privacy notes if using external API). For the PhD demo, likely not an issue but for future, we’d consider an option to run with local models for privacy.

Go-to-Market Plan (first 3 channels):
	1.	Reddit & Forums (Organic): Announce the tool on r/bioinformatics and r/labrats with a short video/gif of it in action. Those communities include exactly the target users who have expressed the problem ￼. This can drive early adopters and feedback. Emphasize that suggestions are evidence-backed and user should verify – to gain trust.
	2.	Twitter (X) & Domain Influencers: Prepare a succinct tweet with an image of an annotated UMAP labeled by our tool and tag known single-cell folks (like developers of Scanpy/Seurat). Many in the single-cell community are active on Twitter and share new tools; using a hashtag like #singlecell or #bioinformatics will increase visibility. Perhaps engage with Chan Zuckerberg Initiative’s cellxgene team via tweet, as our tool complements their data repository.
	3.	Workshops/Slack channels: Reach out on the ScRNA-seq Slack or similar (if available) and share the tool. Also, prepare a short post for Biostars or Bioinformatics StackExchange in a Q&A format (e.g., Q: “How to automate cell type annotation?” A: mention GPT-cell-annotator with a link ￼). This positions it as a solution to an asked question. Additionally, consider a brief demo at a local university’s single-cell interest group or a virtual meetup (if time permits beyond 4 weeks) – but initially online channels suffice.

Admissions Demo Script (3-minute live demo):
(Setting: The candidate is presenting to a faculty panel, demonstrating the tool on a real dataset – for example, a peripheral blood mononuclear cell (PBMC) single-cell dataset which the audience may be familiar with.)
	1.	Intro (30s): “Single-cell biologists often spend days figuring out what cell types are in their data. I built GPT-cell-annotator, an AI assistant that labels cell clusters in seconds, using known biology. Let me show you how it works.” (Shows the web app interface with a title and brief description.)
	2.	Data Input (30s): “Here I have a PBMC dataset with 8 clusters. I’ve calculated the top marker genes for each cluster – something standard from Seurat. I’ll paste those markers into GPT-cell-annotator.” (Copies a list of markers for one cluster into the app, or uploads a file with all clusters’ markers. Clicks ‘Annotate’.)
	3.	Output (60s): “In just a moment, the AI suggests labels. For cluster 0, it says ‘CD14+ Monocytes’ with high confidence – and it explains: ‘because genes CD14, LYZ are highly expressed.’ That’s correct – those are classic monocyte markers ￼. Cluster 1 is labeled ‘Naive CD4 T cells’ – the explanation mentions IL7R and CCR7, markers of naive T cells, which is spot on. Notice it even distinguishes memory vs naive T cells by presence of certain markers. If I click on cluster 2… it shows ‘B cells’ due to CD19, CD79A expression ￼. These matches align with what an immunologist would tell us, meaning the AI is essentially compressing textbook knowledge for us.”
	4.	Uncertainty Handling (30s): “For cluster 5, it says ‘Unknown or Novel’ – why? The markers there are a mix and the AI isn’t confident. That’s actually a good thing: it’s telling us this cluster might not correspond to a well-known cell type. In analysis, that’s a flag to investigate further – perhaps a rare subpopulation. The tool doesn’t force a guess if unsure ￼.” (Shows that output row highlighted or a warning icon.)
	5.	Explanation & Trust (30s): “Importantly, GPT-cell-annotator provides explanations and even literature references. See here, it cited a reference for the monocyte markers. This transparency helps you trust the suggestion and also learn – it’s like a quick tutor on cell identity ￼. Under the hood, it’s powered by a large language model that I fine-tuned on cell type knowledge bases and papers, so it ‘understands’ context like tissue and marker combinations.”
	6.	Wrap-up (20s): “In summary, what used to take me hours of googling and cross-referencing now takes seconds with GPT-cell-annotator. It accelerates single-cell analysis and ensures I don’t miss key cell identities – which is especially critical for complex datasets or unfamiliar tissues. This kind of AI-driven tool can free researchers to focus on biological interpretation rather than manual curation. I plan to expand its knowledge and integrate it with single-cell workflows so that future datasets can essentially auto-annotate themselves, with experts just verifying the suggestions.” (Conclude with the tool’s name/logo on screen and perhaps a tagline: “Know your cells – in a single click.”)

Admissions Value Add: This demo script emphasizes how the product applies advanced AI to a cutting-edge biological problem (aligning with computational biology innovation), and it highlights the candidate’s ability to blend domain knowledge with technical skill. By solving a practical pain point with evidence-based AI suggestions ￼ ￼, it showcases the candidate’s readiness for PhD-level work – dealing with real data challenges and pushing the envelope with novel solutions.